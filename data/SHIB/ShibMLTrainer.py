import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.metrics import classification_report, confusion_matrix
from typing import Optional


# Path to SHIB candles (5-min) generated by your collector
DATA_PATH = Path("Coinbase_SHIBUSD_5min.csv")


def load_shib_data():
    df = pd.read_csv(DATA_PATH)

    # Ensure sorted by time
    if "unix" in df.columns:
        df = df.sort_values("unix").reset_index(drop=True)
    elif "date" in df.columns:
        df["unix"] = pd.to_datetime(df["date"]).astype("int64") // 10**9
        df = df.sort_values("unix").reset_index(drop=True)
    else:
        raise ValueError("Expected 'unix' or 'date' column in SHIB CSV.")

    return df


def engineer_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # Candle body return & close-to-close return
    df["ret_body"] = (df["close"] - df["open"]) / np.where(
        df["open"] == 0, np.nan, df["open"]
    )
    df["ret_cc"] = df["close"].pct_change()
    df["range"] = (df["high"] - df["low"]) / np.where(
        df["open"] == 0, np.nan, df["open"]
    )
    df["vol_change"] = df["volume"].pct_change()

    # 1-day rolling volatility (~288 candles)
    df["volatility_288"] = df["ret_cc"].rolling(288).std()

    # Drop initial NaNs from pct_change/rolling
    df_model = df.dropna().copy()
    return df_model


def create_labels(df_model: pd.DataFrame):
    """Create pump labels using z-score based rule (Option B)."""
    ret_mean, ret_std = (
        df_model["ret_body"].mean(),
        df_model["ret_body"].std(ddof=0),
    )
    vol_mean, vol_std = (
        df_model["vol_change"].mean(),
        df_model["vol_change"].std(ddof=0),
    )

    # Sensitivity "B"
    abs_min_ret = 0.01  # 1% candle body minimum
    z_k = 2.0           # 2Ïƒ on return & volume-change

    label_B = (
        (df_model["ret_body"] > ret_mean + z_k * ret_std)
        & (df_model["vol_change"] > vol_mean + z_k * vol_std)
        & (df_model["ret_body"] > abs_min_ret)
    ).astype(int)
    df_model["pump_label_B"] = label_B
    num_pos_B = int(df_model["pump_label_B"].sum())

    # Fallback: slightly looser thresholds if no positives
    used_sensitivity = "B"
    if num_pos_B == 0:
        z_k_train = 1.5
        abs_min_ret_train = 0.005
        label_train = (
            (df_model["ret_body"] > ret_mean + z_k_train * ret_std)
            & (df_model["vol_change"] > vol_mean + z_k_train * vol_std)
            & (df_model["ret_body"] > abs_min_ret_train)
        ).astype(int)
        used_sensitivity = "C (fallback for training)"
    else:
        label_train = df_model["pump_label_B"]

    df_model["pump_label_train"] = label_train
    num_pos_train = int(df_model["pump_label_train"].sum())
    return df_model, num_pos_B, num_pos_train, used_sensitivity


def time_aware_split(df_model: pd.DataFrame, features):
    X = df_model[features].copy()
    y = df_model["pump_label_train"].copy()

    N = len(df_model)
    min_test = max(1, int(0.1 * N))  # at least 10% test
    default_split = int(0.8 * N)

    # Make sure all positives are inside train
    pos_idx = np.where(y.values == 1)[0]
    if len(pos_idx) > 0:
        last_pos = int(pos_idx.max())
        split_idx = max(default_split, last_pos + 1)
        split_idx = min(split_idx, N - min_test)
    else:
        split_idx = default_split

    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
    return X_train, X_test, y_train, y_test, split_idx


def train_ml_models(df_model: pd.DataFrame, features):
    X_train, X_test, y_train, y_test, split_idx = time_aware_split(
        df_model, features
    )

    reports = {}

    # --- Logistic Regression (scaled) ---
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    if y_train.nunique() >= 2 and y_test.nunique() >= 2:
        lr = LogisticRegression(max_iter=2000, class_weight="balanced")
        lr.fit(X_train_scaled, y_train)
        lr_pred = lr.predict(X_test_scaled)
        lr_proba = lr.predict_proba(X_test_scaled)[:, 1]

        reports["LR_confusion"] = confusion_matrix(y_test, lr_pred).tolist()
        reports["LR_report"] = classification_report(
            y_test, lr_pred, output_dict=True
        )
        df_model.loc[df_model.index[split_idx:], "lr_pred"] = lr_pred
        df_model.loc[df_model.index[split_idx:], "lr_proba"] = lr_proba
    else:
        reports["LR_confusion"] = None
        reports["LR_report"] = None
        df_model["lr_pred"] = np.nan
        df_model["lr_proba"] = np.nan

    # --- Random Forest ---
    if y_train.nunique() >= 2 and y_test.nunique() >= 2:
        rf = RandomForestClassifier(
            n_estimators=400, random_state=42, class_weight="balanced"
        )
        rf.fit(X_train, y_train)
        rf_pred = rf.predict(X_test)
        rf_proba = rf.predict_proba(X_test)[:, 1]

        reports["RF_confusion"] = confusion_matrix(y_test, rf_pred).tolist()
        reports["RF_report"] = classification_report(
            y_test, rf_pred, output_dict=True
        )
        reports["RF_feature_importances"] = dict(
            zip(features, rf.feature_importances_)
        )

        df_model.loc[df_model.index[split_idx:], "rf_pred"] = rf_pred
        df_model.loc[df_model.index[split_idx:], "rf_proba"] = rf_proba
    else:
        reports["RF_confusion"] = None
        reports["RF_report"] = None
        reports["RF_feature_importances"] = None
        df_model["rf_pred"] = np.nan
        df_model["rf_proba"] = np.nan

    # --- Isolation Forest (unsupervised anomalies) ---
    iso = IsolationForest(
        n_estimators=300, contamination=0.01, random_state=42
    )
    X_all = df_model[features].copy()
    iso.fit(X_train)
    iso_score = iso.decision_function(X_all)  # higher = more normal
    iso_pred = iso.predict(X_all)             # -1 anomaly, 1 normal
    df_model["iso_score"] = iso_score
    df_model["iso_anomaly"] = (iso_pred == -1).astype(int)

    sizes = {"train_size": len(X_train), "test_size": len(X_test)}
    return df_model, reports, sizes


# ---------- Pump/dump CSV generator (same idea as PEPE) ----------

def _ensure_features(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    if "date" not in out.columns:
        out["date"] = pd.to_datetime(out["unix"], unit="s", utc=True)
    if "ret_body" not in out.columns:
        out["ret_body"] = (out["close"] - out["open"]) / np.where(
            out["open"] == 0, np.nan, out["open"]
        )
    if "ret_cc" not in out.columns:
        out["ret_cc"] = out["close"].pct_change()
    if "range" not in out.columns:
        out["range"] = (out["high"] - out["low"]) / np.where(
            out["open"] == 0, np.nan, out["open"]
        )
    if "vol_change" not in out.columns:
        out["vol_change"] = out["volume"].pct_change()
    if "volatility_288" not in out.columns:
        out["volatility_288"] = out["ret_cc"].rolling(288).std()
    return out.dropna().copy()


def _thresholds_for_sensitivity(sensitivity: str):
    s = sensitivity.upper()
    if s == "A":
        return 3.0, 0.02   # strict
    elif s == "C":
        return 1.5, 0.005  # loose
    else:
        return 2.0, 0.01   # default "B"


def _zscore_series(x: pd.Series, window: Optional[int] = None):
    if window is None:
        mu, sig = x.mean(), x.std(ddof=0)
        return (x - mu) / (sig if sig > 0 else 1.0)
    else:
        mu = x.rolling(window).mean()
        sig = x.rolling(window).std(ddof=0)
        sig = sig.replace(0, np.nan)
        return (x - mu) / sig


def generate_event_reports(
    df: pd.DataFrame,
    sensitivity: str = "B",
    rolling_window: Optional[int] = None,
    lookahead_minutes: int = 12 * 60,
):
    """
    Rule-based pump/dump detector that writes:
      - shib_pumps_by_candle.csv
      - shib_dumps_by_candle.csv
      - shib_pumps_by_day.csv
      - shib_dumps_by_day.csv
      - shib_pump_then_dump_sequences.csv
    """
    dfx = _ensure_features(df).sort_values("unix").reset_index(drop=True)

    z_k, min_abs_ret = _thresholds_for_sensitivity(sensitivity)

    z_ret = _zscore_series(dfx["ret_body"], rolling_window)
    z_vol = _zscore_series(dfx["vol_change"], rolling_window)

    pump_mask = (z_ret > z_k) & (z_vol > z_k) & (dfx["ret_body"] > min_abs_ret)
    dump_mask = (z_ret < -z_k) & (z_vol > z_k) & (dfx["ret_body"] < -min_abs_ret)

    pumps_df = dfx.loc[
        pump_mask,
        ["date", "open", "high", "low", "close", "volume", "ret_body", "vol_change"],
    ].copy()
    dumps_df = dfx.loc[
        dump_mask,
        ["date", "open", "high", "low", "close", "volume", "ret_body", "vol_change"],
    ].copy()

    # Per-day aggregates
    if not pumps_df.empty:
        pumps_df["day"] = pumps_df["date"].dt.date
        pumps_by_day = (
            pumps_df.groupby("day")
            .agg(
                first_time=("date", "min"),
                last_time=("date", "max"),
                count=("date", "count"),
                max_ret=("ret_body", "max"),
                max_volchg=("vol_change", "max"),
            )
            .reset_index()
            .sort_values("first_time")
        )
    else:
        pumps_by_day = pd.DataFrame(
            columns=["day", "first_time", "last_time", "count", "max_ret", "max_volchg"]
        )

    if not dumps_df.empty:
        dumps_df["day"] = dumps_df["date"].dt.date
        dumps_by_day = (
            dumps_df.groupby("day")
            .agg(
                first_time=("date", "min"),
                last_time=("date", "max"),
                count=("date", "count"),
                min_ret=("ret_body", "min"),
                max_volchg=("vol_change", "max"),
            )
            .reset_index()
            .sort_values("first_time")
        )
    else:
        dumps_by_day = pd.DataFrame(
            columns=["day", "first_time", "last_time", "count", "min_ret", "max_volchg"]
        )

    # Link pump -> first dump within lookahead window
    sequences = []
    if not pumps_df.empty and not dumps_df.empty:
        for _, r in pumps_df.iterrows():
            after = dumps_df[dumps_df["date"] > r["date"]]
            within = after[
                after["date"]
                <= r["date"] + pd.to_timedelta(lookahead_minutes, unit="m")
            ]
            if not within.empty:
                first = within.iloc[0]
                sequences.append(
                    {
                        "pump_time": r["date"],
                        "pump_ret": r["ret_body"],
                        "dump_time": first["date"],
                        "dump_ret": first["ret_body"],
                        "minutes_between": (
                            first["date"] - r["date"]
                        ).total_seconds()
                        / 60.0,
                    }
                )

    seq_df = pd.DataFrame(
        sequences,
        columns=["pump_time", "pump_ret", "dump_time", "dump_ret", "minutes_between"],
    )
    if not seq_df.empty:
        seq_df = seq_df.sort_values("pump_time")

    pumps_path = "shib_pumps_by_candle.csv"
    dumps_path = "shib_dumps_by_candle.csv"
    pumps_day_path = "shib_pumps_by_day.csv"
    dumps_day_path = "shib_dumps_by_day.csv"
    seq_path = "shib_pump_then_dump_sequences.csv"

    pumps_df.to_csv(pumps_path, index=False)
    dumps_df.to_csv(dumps_path, index=False)
    pumps_by_day.to_csv(pumps_day_path, index=False)
    dumps_by_day.to_csv(dumps_day_path, index=False)
    seq_df.to_csv(seq_path, index=False)

    print(
        {
            "sensitivity": sensitivity,
            "rolling_window": rolling_window,
            "pumps_found": int(len(pumps_df)),
            "dumps_found": int(len(dumps_df)),
            "days_with_pumps": int(len(pumps_by_day)),
            "days_with_dumps": int(len(dumps_by_day)),
            "sequences_found": int(len(seq_df)),
            "files": {
                "pumps_by_candle": pumps_path,
                "dumps_by_candle": dumps_path,
                "pumps_by_day": pumps_day_path,
                "dumps_by_day": dumps_day_path,
                "pump_then_dump_sequences": seq_path,
            },
        }
    )


def main():
    print("Loading SHIB data from", DATA_PATH)
    df = load_shib_data()
    print(f"Loaded {len(df)} candles.")

    print("Engineering features...")
    df_model = engineer_features(df)
    print(f"After dropping NaNs, {len(df_model)} candles remain for modeling.")

    df_model, num_pos_B, num_pos_train, used_sensitivity = create_labels(df_model)
    print(f"Positives (Option B rule): {num_pos_B}")
    print(
        f"Positives used for training: {num_pos_train} "
        f"(sensitivity {used_sensitivity})"
    )

    features = ["ret_body", "ret_cc", "range", "vol_change", "volatility_288"]

    print("Time-aware train/test split and model training...")
    df_model, reports, sizes = train_ml_models(df_model, features)

    print("Training summary:")
    print("  Train size:", sizes["train_size"])
    print("  Test size :", sizes["test_size"])
    print("  LR confusion:", reports["LR_confusion"])
    print("  RF confusion:", reports["RF_confusion"])
    print("  RF feature importances:", reports["RF_feature_importances"])

    # Save per-candle ML outputs
    out_cols = [
        "unix",
        "open",
        "high",
        "low",
        "close",
        "volume",
        "ret_body",
        "ret_cc",
        "range",
        "vol_change",
        "volatility_288",
        "pump_label_B",
        "pump_label_train",
        "lr_pred",
        "lr_proba",
        "rf_pred",
        "rf_proba",
        "iso_score",
        "iso_anomaly",
    ]
    out_cols = [c for c in out_cols if c in df_model.columns]
    out_df = df_model[out_cols].copy()
    out_path = "SHIB_5min_pump_predictions.csv"
    out_df.to_csv(out_path, index=False)
    print("Saved per-candle ML outputs to", out_path)

    print("Generating rule-based pump/dump CSVs for SHIB...")
    generate_event_reports(df, sensitivity="B", rolling_window=None)


if __name__ == "__main__":
    main()
